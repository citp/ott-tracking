import json
try:
    from urllib2 import urlopen
except ImportError:
    from urllib.request import urlopen

import pandas as pd
import ipaddress
try:
    from urlparse import urlparse
except ImportError:
    from urllib.parse import urlparse

from tld import get_fld
from glob import glob
from os.path import join, sep, isdir


TSHARK_FIELD_SEP = "|"
ROKU_MACS = ["d8:31:34:22:e6:ff"]  # Roku MAC addresses to filter packets

CRAWL_ROOT_DIRS = [
    '/mnt/iot-house/crawl-data/',
    '/home/gacar/dev/smart-tv/data',
     '/media/gacar/Data/iot-house/crawl-data/',
    ]

def get_crawl_data_path(crawl_name):
    for crawl_root_dir in CRAWL_ROOT_DIRS:
        crawl_dir_path = join(crawl_root_dir, crawl_name)
        if isdir(crawl_dir_path):
            return crawl_dir_path
    else:
        raise Exception("Cannot find the crawl data dir %s" % crawl_name)

def get_ps1_or_ipaddress(url):
    try:
        return get_fld(url, fail_silently=False)
    except Exception:
        hostname = urlparse(url).hostname
        try:
            ipaddress.ip_address(hostname)
            return hostname
        except Exception:
            return None


def read_extracted_fields(csv_path):
    """Read fields from txt files generated by tshark."""
    for l in open(csv_path):
        yield l.rstrip().split(TSHARK_FIELD_SEP)


def parse_roku_output_filename(filename):
    """Parse file names, which follows a specific pattern."""
    parts = filename.replace(".pcap.txt", "").split("-")
    channel_id, start_ts, command = parts[0:3]
    select_idx = parts[3] if command == "select" else 0
    return int(channel_id), int(start_ts), command, int(select_idx)


def read_pcap_fields_from_csvs(csv_dir, suffix=".csv"):
    for csv_path in glob(join(csv_dir, suffix)):
        filename = csv_path.split(sep)[-1]
        filename_wo_extension = filename.split(".")[0]
        # channel_id, start_ts, command, select_idx = parse_roku_output_filename(filename)
        channel_id, start_ts = filename_wo_extension.split("-")
        for pcap_fields in read_extracted_fields(csv_path):
            yield [channel_id, start_ts] + pcap_fields


# Download and parse Roku channel json - From Danny's notebook
CHANNEL_DATA_URL = 'https://iot-inspector.princeton.edu/pcaps/roku-channel-surfer/channel-list.txt'


def download_roku_channel_details(channel_data_url=CHANNEL_DATA_URL):
    channel_df = []
    for channel in urlopen(CHANNEL_DATA_URL).read().split('\n'):
        if channel:
            channel_df.append(json.loads(channel))
    return pd.DataFrame(channel_df).set_index('id').sort_values("rankByWatched")


def read_channel_details_df():
    channel_df = []
    ROKU_CATEGORY_DIR = "../../../scrape/platforms/roku/channel_lists/categories/"
    for category_txt in glob(join(ROKU_CATEGORY_DIR, "*.txt")):
        for channel_json_str in open(category_txt):
            channel_df.append(json.loads(channel_json_str))
    
    ROKU_OLD_CHANNEL_LIST = "../../../legacy-code/roku_readonly/channel_list_readonly.txt"
    ROKU_KIDS_AND_TV_CHANNELS = "../../../scrape/platforms/roku/channel_lists/all_channel_list.txt"
    for channel_list_file in [ROKU_OLD_CHANNEL_LIST, ROKU_KIDS_AND_TV_CHANNELS]:
        for channel_json_str in open(channel_list_file):
            channel_df.append(json.loads(channel_json_str))
        
    roku_df = pd.DataFrame(channel_df)
    roku_df.rename(columns={'id': 'channel_id',
                            'rankByWatched': 'rank',
                            '_category': 'category',
                            'name': 'channel_name'}, inplace=True)
    roku_df['channel_id'] = roku_df['channel_id'].astype(str)

    roku_df = roku_df.drop_duplicates('channel_id').set_index('channel_id').sort_values(["category", "rank"])
    # print(roku_df.columns)
    roku_df.drop(['_scrape_ts', 'accessCode', 'datePublished', 'desc', 'thumbnail'], inplace=True, axis=1)
    roku_df['platform'] = 'roku'
    AMAZON_CHANNEL_DETAILS_CAT_CSV = "../../../scrape/platforms/amazon/channel_details/apk_info_cat.csv"
    AMAZON_CHANNEL_DETAILS_CSV = "../../../scrape/platforms/amazon/channel_details/apk_info.csv"
    AMAZON_CHANNEL_DETAILS_100_RANDOM_CSV = "../../../scrape/platforms/amazon/channel_lists/100-channel_name.csv"
    amazon_df = pd.read_csv(AMAZON_CHANNEL_DETAILS_CAT_CSV)
    amazon_df = amazon_df.append(pd.read_csv(AMAZON_CHANNEL_DETAILS_CSV), sort=True)
    tmp_df = pd.read_csv(AMAZON_CHANNEL_DETAILS_100_RANDOM_CSV, comment='#')
    # print(amazon_df.columns)
    # print(tmp_df.columns)
    amazon_df = amazon_df.append(tmp_df.rename({"channel_name": "product_name"}, axis=1), sort=True)
    # print(amazon_df.columns)
    amazon_df.rename(columns={'amazon_ranking': 'rank',
                              'amazon_category': 'category',
                              'apk_id': 'channel_id',
                              'product_name': 'channel_name'}, inplace=True)
    amazon_df['channel_id'] = amazon_df['channel_id'].astype(str)
    # print(amazon_df.columns)
    amazon_df = amazon_df.drop_duplicates('channel_id').set_index('channel_id').sort_values(["category", "rank"])
    amazon_df.drop(['product_id', 'apk_name', 'apk_name_matches_product_name',
                    'overlap_token_count', 'developer_name'], inplace=True, axis=1)
    amazon_df['platform'] = 'amazon'
    # print(amazon_df.columns)
    return roku_df.append(amazon_df, sort=True)


if __name__ == '__main__':
    df = read_channel_details_df()
    print(len(df))
